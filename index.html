<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Visual Instruction Tuning">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GLaMM</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
</head>


<style>
    .section {
    margin-bottom: -30px; /* Adjust this value as needed to reduce the space */
  }
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="images/logos/face.png" alt="GLaMM_face" width="100">
            <h1 class="title is-1 publication-title">GLaMM: <span class="is-size-2"><span class="is-size-1">G</span>rounding <span class="is-size-1">L</span>arge <span class="is-size-1">M</span>ultimodal <span class="is-size-1">M</span>odel</span></h1>
            <div class="is-size-5 publication-authors">
              <!-- First Group of 3 Authors -->
              <div class="author-group">
                  <span class="author-block">
                      <a href="https://www.hanoonarasheed.com" style="color:#f68946;font-weight:normal;">Hanoona Rasheed<sup>*</sup></a>,
                  </span>
                  <span class="author-block">
                      <a href="https://www.muhammadmaaz.com/" style="color:#008AD7;font-weight:normal;">Muhammad Maaz<sup>*</sup></a>,
                  </span>
              </div>

              <!-- Second Group of 3 Authors -->
              <div class="author-group">
                  <span class="author-block">
                      <a href="https://www.linkedin.com/in/sahalshajim" style="color:#F2A900;font-weight:normal;">Sahal Shaji</a>,
                  </span>
                  <span class="author-block">
                      <a href="https://amshaker.github.io" style="color:#f68946;font-weight:normal;">Abdelrahman Shaker</a>,
                  </span>
                  <span class="author-block">
                      <a href="https://salman-h-khan.github.io" style="color:#f68946;font-weight:normal;">Salman Khan</a>,
                  </span>
              </div>

              <!-- Third Group of 4 Authors -->
              <div class="author-group">
                  <span class="author-block">
                      <a href="https://scholar.google.ae/citations?user=bZ3YBRcAAAAJ&hl=fr" style="color:#f68946;font-weight:normal;">Hisham Cholakkal</a>,
                  </span>
                  <span class="author-block">
                      <a href="https://scholar.google.fi/citations?user=_KlvMVoAAAAJ&hl=en" style="color:#f68946;font-weight:normal;">Rao M. Anwer</a>,
                  </span>
                  <span class="author-block">
                      <a href="https://www.cs.cmu.edu/~epxing" style="color:#f68946;font-weight:normal;">Eric Xing</a>,
                  </span>
                  <span class="author-block">
                      <a href="https://scholar.google.com.pk/citations?user=p9-ohHsAAAAJ&hl=en" style="color:#f68946;font-weight:normal;">Ming-Hsuan Yang</a>,
                  </span>
                  <span class="author-block">
                      <a href="https://sites.google.com/view/fahadkhans/home" style="color:#f68946;font-weight:normal;">Fahad S. Khan</a>
                  </span>
              </div>
          </div>
            <div class="is-size-5 publication-authors">
              Mohamed bin Zayed University of AI (MBZUAI)<br>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equally contributing first authors</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="EDIT_HERE_PLEASE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/mbzuai-oryx/groundingLMM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#grand-dataset" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-justified">
          Grounding Large Multimodal Model (GLaMM) is an end-to-end trained LMM which provides visual grounding capabilities with the flexibility to process both image and region inputs. This enables the new unified task of Grounded Conversation Generation that combines phrase grounding, referring expression segmentation and vision-language conversations.  Equipped with the capability for detailed region understanding, pixel-level groundings, and conversational abilities, GLaMM offers a versatile capability to interact with visual inputs provided by the user at multiple granularity levels (objects, object parts, attributes, relationships and holistic scene understanding).
        </h4>
      </div>
    </div>
  </section>


<div style="text-align:center;">
  <iframe width="1024" height="720" src="https://www.youtube.com/embed/lTvi8BspR3o" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>



  <section class="section"  style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">ðŸ”¥Highlights</h2>
        <div class="content has-text-justified">
          <ol type="1">
            <li><b>GLaMM Introduction</b>. We present the <span style="color: #997300;">Grounding Large Multimodal Model (GLaMM)</span>, the first-of-its-kind model capable of generating natural language responses that are seamlessly integrated with object segmentation masks. Unlike existing models, GLaMM accommodates both textual and optional visual prompts, facilitating enhanced multimodal user interaction. </li>
              <br>
            <li><b>Novel Task & Evaluation</b>. Recognizing the lack of standardized benchmarks for visually grounded conversations, we propose a new task of <span style="color: #997300;">Grounded Conversation Generation</span> (GCG). Alongside, we introduce a comprehensive evaluation protocol to measure the efficacy of models in this novel setting, filling a significant gap in the literature.</li>
              <br>
            <li><b>GranD Dataset Creation</b>. To facilitate model training and evaluation, we create <span style="color: #997300;">GranD - Grounding-anything Dataset</span>, a large-scale densely annotated dataset. Developed using an automatic annotation pipeline and verification criteria, it encompasses 7.5M unique concepts grounded in 810M regions. Additionally, we produce <span style="color: #997300;">GranD-f</span>, a high-quality dataset explicitly designed for the GCG task, by re-purposing existing open-source datasets.</li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>



<!--Model Arch-->
<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img src="images/glamm/GlamImage-min.png" alt="GLaMM_face" width="60" style="vertical-align: bottom;"> GLaMM: Grounding Large Multimodal Model</h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            GLaMM consists of five core components to achieve visually grounded conversations: i) Global Image Encoder, ii) Region Encoder, iii) LLM, iv) Grounding Image Encoder, and v) Pixel Decoder. These components are cohesively designed to handle both textual and optional visual prompts (image level and region of interest), allowing for interaction at multiple levels of granularity, and generating grounded text responses.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="images/glamm/model_arch.png">
          <figcaption>
            The figure illustrates our model architecture showcasing its ability to offer scene-level understanding, region-level interpretation, and pixel-level grounding. The bottom row shows the diverse downstream applications of GLaMM, including referring expression segmentation, region-level captioning, image-level captioning and phrase grounding.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>
<!-- Model Arch -->
<!--Dataset-->
<section id="grand-dataset" class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img src="images/glamm/GlamImage-min.png" alt="GLaMM_face" width="70" style="vertical-align: bottom;"> Grounding-anything Dataset (GranD)</h2>
    </div>
  </div>
  <!--Dataset Pipeline-->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Detailed region-level understanding requires the laborious process of collecting large-scale annotations for image regions. To alleviate the manual labelling effort, we propose an automated pipeline to annotate the large-scale Grounding-anything Dataset. Leveraging the automated pipeline with dedicated verification steps, GranD comprises <span style="color: #997300;">7.5M unique concepts</span> anchored in a total of <span style="color: #997300;">810M regions</span>, each with a segmentation mask.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="images/glamm/dataset_pipeline.png">
          <figcaption>
            The figure shows the annotation Pipeline of the GranD Dataset. Level-1 details objects and attributes, level-2 includes short captions and relational markers, level-3 builds a scene graph, hierarchically organizing information from earlier levels to facilitate LLM for grounded dense captions, level-4 provides additional historical and societal context for a richer visual understanding.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
    <br>
  <!--Dataset Samples-->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Below we present some examples of the GranD dataset. Our automated annotation pipeline provides multiple semantic tags and attributes for objects along with segmentation masks. The dense caption thoroughly describes the visual scene with part of the text grounded to the corresponding objects. The additional context provides a deeper understanding of the scene, going beyond what's observed.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="images/glamm/grand_sample_2.png">
<!--          <figcaption>-->
<!--            Dataset sample from GranD dataset-->
<!--          </figcaption>-->
        </figure>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="images/glamm/grand_sample_1.png">
          <figcaption>
            Dataset samples from GranD dataset.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
    <br>
    <br>
    <!--Subsection-->
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h3 class="title is-4">Building GranD-f for Grounded Conversation Generation</h3>
      <div class="content has-text-justified">
        <p>
          Motivated by the need for higher-quality data in fine-tuning stage, we introduce GranD-f. Explicitly designed for the GCG task, this dataset encompasses approximately 214K image-grounded text pairs. Of these, 2.6k samples are reserved for validation and 5k for testing. GranD-f comprises two primary components: one subset is manually annotated and the other subset derived by re-purposing existing open-source datasets.
        </p>
      </div>
    </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <figure style="text-align: center;">
        <img id="annotation_example" width="100%" src="images/glamm/grand_f_samples.png">
<!--        <figcaption>-->
<!--          Dataset samples from GranD-f.-->
<!--        </figcaption>-->
      </figure>
    </div>
  </div>
</div>


</section>
<!--Dataset-->
<!--Results-->
<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img src="images/glamm/GlamImage-min.png" alt="GLaMM_face" width="70" style="vertical-align: bottom;"> Grounded Conversation Generation (GCG)</h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            The objective of the GCG task is to construct image-level captions with specific phrases directly tied to corresponding segmentation masks in the image. By introducing the GCG task, we bridge the gap between textual and visual understanding, thereby enhancing the modelâ€™s ability for fine-grained visual grounding alongside natural language captioning.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="images/glamm/results_7_gcg_combined.png">
          <figcaption>
            Qualitative results of GLaMM's performance in grounded conversation generation.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>

  <!--Table - GCG-->
  <div style="text-align: center;">
    <table border="1" cellspacing="0" style="width: 58%; border-collapse: collapse; text-align: center; margin: 0 auto;">
        <caption><strong>Comparison of GLaMM Model Performance on GCG Task</strong>: Metrics include METEOR, CIDEr, AP, mIoU, and Mask Recall for both validation and test sets in our proposed benchmark. LISA* indicates a modified LISA adapted for GCG.</caption>
        <thead>
            <tr>
                <th rowspan="2">Model</th>
                <th colspan="5">Validation Set</th>
                <th colspan="5">Test Set</th>
            </tr>
            <tr>
                <th>METEOR</th>
                <th>CIDEr</th>
                <th>AP50</th>
                <th>mIoU</th>
                <th>Recall</th>
                <th>METEOR</th>
                <th>CIDEr</th>
                <th>AP50</th>
                <th>mIoU</th>
                <th>Recall</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>BuboGPT</td>
                <td>16.2</td>
                <td>2.6</td>
                <td>4.3</td>
                <td>29.9</td>
                <td>8.6</td>
                <td>15.9</td>
                <td>2.2</td>
                <td>4.5</td>
                <td>28.9</td>
                <td>8.7</td>
            </tr>
            <tr>
                <td>Kosmos-2</td>
                <td>16.3</td>
                <td>19.6</td>
                <td>15.5</td>
                <td>57.4</td>
                <td>33.0</td>
                <td>16.2</td>
                <td>17.8</td>
                <td>15.6</td>
                <td>58.3</td>
                <td>34.3</td>
            </tr>
            <tr>
                <td>LISA*</td>
                <td>13.8</td>
                <td>49.8</td>
                <td>27.2</td>
                <td>65.5</td>
                <td>42.0</td>
                <td>13.8</td>
                <td>48.9</td>
                <td>28.4</td>
                <td>66.3</td>
                <td>43.2</td>
            </tr>
            <tr style="background-color: #e6e6ff;">
                <td>GLaMM</td>
                <td>14.2</td>
                <th>52.6</th>
                <th>28.0</th>
                <th>65.7</th>
                <th>42.6</th>
                <th>13.9</th>
                <th>49.0</th>
                <th>28.8</th>
                <th>67.1</th>
                <th>45.5</th>
            </tr>
        </tbody>
    </table>
  </div>

</section>
<!--Results -->
<!--Downstream-->
<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img src="images/glamm/GlamImage-min.png" alt="GLaMM_face" width="70" style="vertical-align: bottom;"> Downstream Applications</h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <!-- First Subheading -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Referring Expression Segmentation</h3>
        <div class="content has-text-justified">
          <p>
            In this task, the model receives an image along with a text-based referring expression, to which it outputs a corresponding segmentation mask. We present quantitative results on the validation and testing sets of refCOCO, refCOCO+, and refCOCOg.
          </p>
        </div>
        <figure>
          <img width="100%" src="images/glamm/results_3_refseg.png">
          <figcaption style="text-align: center;">Qualitative results of GLaMM's capability in referring expression segmentation.</figcaption>
        </figure>
      </div>
    </div>
      <div style="text-align: center;">
    <table border="1" cellspacing="0" style="width: 100%; border-collapse: collapse; text-align: center; margin: 0 auto;">
        <caption><strong>Quantitative Assessment of GLaMM in Referring-Expression Segmentation</strong>: Performance across refCOCO, refCOCO+, and refCOCOg in generating accurate segmentation masks based on text-based referring expressions surpasses that of closely related work.</caption>
        <thead>
            <tr>
                <th rowspan="2">Method</th>
                <th colspan="3">refCOCO</th>
                <th colspan="3">refCOCO+</th>
                <th colspan="2">refCOCOg</th>
            </tr>
            <tr>
                <th>val</th>
                <th>testA</th>
                <th>testB</th>
                <th>val</th>
                <th>testA</th>
                <th>testB</th>
                <th>val(U)</th>
                <th>test(U)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>CRIS (CVPR-22)</td>
                <td>70.5</td>
                <td>73.2</td>
                <td>66.1</td>
                <td>65.3</td>
                <td>68.1</td>
                <td>53.7</td>
                <td>59.9</td>
                <td>60.4</td>
            </tr>
            <tr>
                <td>LAVT (CVPR-22)</td>
                <td>72.7</td>
                <td>75.8</td>
                <td>68.8</td>
                <td>62.1</td>
                <td>68.4</td>
                <td>55.1</td>
                <td>61.2</td>
                <td>62.1</td>
            </tr>
            <tr>
                <td>GRES (CVPR-23)</td>
                <td>73.8</td>
                <td>76.5</td>
                <td>70.2</td>
                <td>66.0</td>
                <td>71.0</td>
                <td>57.7</td>
                <td>65.0</td>
                <td>66.0</td>
            </tr>
            <tr>
                <td>X-Decoder (CVPR-23)</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>64.6</td>
                <td>-</td>
            </tr>
            <tr>
                <td>SEEM (arXiv-23)</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>65.7</td>
                <td>-</td>
            </tr>
            <tr>
                <td>LISA-7B (ZS) (arXiv-23)</td>
                <td>74.1</td>
                <td>76.5</td>
                <td>71.1</td>
                <td>62.4</td>
                <td>67.4</td>
                <td>56.5</td>
                <td>66.4</td>
                <td>68.4</td>
            </tr>
            <tr>
                <td>LISA-7B (FT) (arXiv-23)</td>
                <td>74.9</td>
                <td>79.1</td>
                <td>72.3</td>
                <td>65.1</td>
                <td>70.8</td>
                <td>58.1</td>
                <td>67.9</td>
                <td>70.6</td>
            </tr>
            <tr style="background-color: #e6e6ff;">
                <td>GLaMM (ZS)</td>
                <td>54.7</td>
                <td>58.1</td>
                <td>52.2</td>
                <td>42.5</td>
                <td>47.1</td>
                <td>39.5</td>
                <td>54.8</td>
                <td>55.6</td>
            </tr>
            <tr style="background-color: #e6e6ff;">
                <td>GLaMM (FT)</td>
                <th>78.3</th>
                <th>81.5</th>
                <th>74.4</th>
                <th>68.0</th>
                <th>75.7</th>
                <th>61.8</th>
                <th>72.5</th>
                <th>72.0</th>
            </tr>
        </tbody>
    </table>
    </div>
      <br><br>


    <!-- Second Subheading -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Region-Level Captioning</h3>
        <div class="content has-text-justified">
          <p>
            In this task, the goal is to generate referring expressions, or region-specific captions. The model is input an image, a designated region, and accompanying text, and is then tasked with responding to questions about the specified region. We conduct a quantitative assessment of region-level captioning on two OpenSource datasets: Visual Genome and refCOCOg. The qualitative results shown below demonstrates GLaMM's ability to adeptly generate region-specific captions, translating the intricate details from designated regions into coherent textual descriptions, enriched by its training on the comprehensive GranD dataset. This capability, combined with the inherent reasoning abilities of LLMs, enables it to tackle reasoning-based visual questions about these regions.
          </p>
        </div>
        <figure>
          <img width="100%" src="images/glamm/results_4_regcap.png">
          <figcaption style="text-align: center;">Qualitative illustration of GLaMM's performance in region-level captioning.</figcaption>
        </figure>
      </div>
    </div>
<!--      results-->
      <div style="text-align: center;">
    <table border="1" cellspacing="0" style="width: 80%; border-collapse: collapse; text-align: center; margin: 0 auto;">
        <caption><strong>Performance of GLaMM in Region-Level Captioning</strong>: Metrics include METEOR and CIDEr scores, assessed on Visual Genome and refCOCOg Datasets, exhibiting competitive results.</caption>
        <thead>
            <tr>
                <th rowspan="2">Model</th>
                <th colspan="2">refCOCOg</th>
                <th colspan="2">Visual Genome</th>
            </tr>
            <tr>
                <th>METEOR</th>
                <th>CIDEr</th>
                <th>METEOR</th>
                <th>CIDEr</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>GRIT</td>
                <td>15.2</td>
                <td>71.6</td>
                <td>17.1</td>
                <td>142</td>
            </tr>
            <tr>
                <td>Kosmos-2</td>
                <td>14.1</td>
                <td>62.3</td>
                <td>-</td>
                <td>-</td>
            </tr>
            <tr>
                <td>GPT4RoI</td>
                <td>-</td>
                <td>-</td>
                <td>17.4</td>
                <td>145.2</td>
            </tr>
            <tr style="background-color: #e6e6ff;">
                <td>GLaMM (ZS)</td>
                <td>15.7</td>
                <td>104.0</td>
                <td>17.0</td>
                <td>127.0</td>
            </tr>
            <tr style="background-color: #e6e6ff;">
                <td>GLaMM (FT)</td>
                <th>16.2</th>
                <th>105.0</th>
                <th>18.6</th>
                <th>157.8</th>
            </tr>
        </tbody>
    </table>
</div>
<br><br>

    <!-- Third Subheading -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Image Captioning</h3>
        <div class="content has-text-justified">
          <p>
             GLaMM offers favourable performance when compared with recent models specialized in image captioning, as well as other LMMs. Qualitative results for image captioning are shown below.
          </p>
        </div>
        <figure>
          <img width="100%" src="images/glamm/results_6_cap.png">
          <figcaption style="text-align: center;">Qualitative results of GLaMM on image-level captioning tasks..</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>
<!--Downstream -->
<!--Conv-->
<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img src="images/glamm/GlamImage-min.png" alt="GLaMM_face" width="70" style="vertical-align: bottom;"> Conversational Style Question Answering</h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            We evaluate our model on conversational style question answering. The qualitative results shown below showcases GLaMM engaging in multi-turn dialogues, providing detailed descriptions, addressing region-specific inquiries, and presenting grounded conversations. This effectively highlights its adaptability in intricate visual-language interactions and robustly retaining reasoning capabilities inherent to LLMs.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="images/glamm/results_4_conv.png">
          <figcaption>
          </figcaption>
        </figure>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="images/glamm/results_5_conv.png">
          <figcaption>
            Multimodal conversational interactions facilitated by GLaMM.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>
<!--Conv -->
<style>
  #BibTeX {
    margin-bottom: -80px; /* Adjust the negative margin as needed */
  }
  #Acknowledgement {
    margin-top: -80px; /* Adjust the negative margin as needed */
  }
</style>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
  @article{hanoona2023GLaMM,
          author={Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal,
          Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, Fahad S. Khan},
          title={GLaMM: Grounding Large Multimodal Model},
          publisher={arXiv:EDIT_HERE_PLEASE},
          year={2023},
  }
  </code></pre>
    </div>
  </section>
  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.  We are thankful to LLaVA, GPT4ROI, and LISA for releasing their models and code as open-source contributions.
      </p>
    </div>
  </section>

</body>

</html>

<div style="text-align: center;">
  <a href="https://www.ival-mbzuai.com" target="_blank">
    <img src="images/logos/IVAL_logo.png" width="200" height="100" alt="IVAL Logo">
  </a>
  <a href="https://github.com/mbzuai-oryx" target="_blank">
    <img src="images/logos/Oryx_logo.png" width="100" height="100" alt="Oryx Logo">
  </a>
  <a href="https://mbzuai.ac.ae" target="_blank">
    <img src="images/logos/MBZUAI_logo.png" width="360" height="85" alt="MBZUAI Logo">
  </a>
</div>

